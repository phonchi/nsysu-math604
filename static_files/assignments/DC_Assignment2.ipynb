{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "DC_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2"
      ],
      "metadata": {
        "id": "ZguKQsaEtw45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student ID: *Double click here to fill the Student ID*\n",
        "\n",
        "#### Name: *Double click here to fill the name*"
      ],
      "metadata": {
        "id": "RxvUeukG506E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Temperature forecasting\n",
        "\n",
        "In this question, we will tackle the prediction problem of a multivariate time series. We will use the Jena Climate dataset recorded by the\n",
        "[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/). In this dataset, 14 different quantities (such as temperature, pressure, humidity, wind direction, and so on) were recorded every 10 minutes over several years. We will only use a subset of features to speed up the training:\n",
        "\n",
        "Index| Features      |Format             |Description\n",
        "-----|---------------|-------------------|-----------------------\n",
        "1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n",
        "2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n",
        "3    |T (degC)       |-8.02              |Temperature in Celsius\n",
        "4    |Tpot (K)       |265.4              |Temperature in Kelvin\n",
        "5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n",
        "6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n",
        "7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n",
        "8    |VPact (mbar)   |3.11               |Vapor pressure\n",
        "9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n",
        "10   |sh (g/kg)      |1.94               |Specific humidity\n",
        "11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n",
        "12   |rho (g/m ** 3) |1307.75            |Airtight\n",
        "13   |wv (m/s)       |1.03               |Wind speed\n",
        "14   |max. wv (m/s)  |1.75               |Maximum wind speed\n",
        "15   |wd (deg)       |152.3              |Wind direction in degrees\n",
        " \n",
        "\n",
        "The exact formulation of the problem is as follows: **Try to predict the temperature 24 hours in the future, given a time series of hourly measurements of quantities recorded over the past 5 days** by a set of sensors. \n",
        "\n",
        "\n",
        "Hint: Notice that the Recurrent models with very few parameters, like the ones in this assignment, tend to be significantly faster on a multicore CPU than on GPU because they only involve small matrix multiplications, and the chain of multiplications is not well parallelizable due to the presence of a for loop (But larger RNNs can significantly benefit from a GPU runtime). Therefore, try to train with 1 or 2 epochs to test the correctness of your code before training it with larger epochs. "
      ],
      "metadata": {
        "id": "FZWBn2fiCh0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, use the following code snippet to preprocess the dataset."
      ],
      "metadata": {
        "id": "w9iXYvI7gHDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "df = pd.read_csv(\"climate.csv\")\n",
        "\n",
        "# Select a subset of features as our predictor (Notice the predictor contains temperature) and temperature as our target\n",
        "temperature = df.iloc[:,2]\n",
        "raw_data = df.iloc[:,[1, 2, 6, 8, 9, 11, 12]]\n",
        "\n",
        "# We’ll use the first 20% of the data for training, the following 20% for validation, and the last 60% for testing\n",
        "num_train_samples = int(0.2 * len(raw_data))\n",
        "num_val_samples = int(0.2 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "\n",
        "# Normalize the data\n",
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std\n",
        "\n",
        "# Use timeseries_dataset_from_array() to generate input and target on the fly\n",
        "# `sampling_rate = 6`— Observations will be sampled at one data point per hour: we will only keep one data point out of 6.\n",
        "# `seq_length = 120`— Observations will go back 5 days (120 hours).\n",
        "# `delay = sampling_rate * (sequence_length + 24 - 1)`— The target for a sequence will be the temperature 24 hours after the end of the sequence.\n",
        "\n",
        "sampling_rate = 6\n",
        "seq_length = 120\n",
        "delay = sampling_rate * (seq_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    data=raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=seq_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples)\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    data=raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=seq_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples)\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    data=raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=seq_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples)"
      ],
      "metadata": {
        "id": "_ArDmIuhJpdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Try to predict the temperature 24 hours later using the naive approach (i.e., the temperature 24 hours from now will be equal to the temperature right now). Report the Mean absolute error (MAE) in the unit of degrees Celsius.\n",
        "\n",
        "Hint: Refer to https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array for more details"
      ],
      "metadata": {
        "id": "9batOLKbJmLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "l5Q6xHq6ggOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Build an RNN with one layer of 16 simple cells, followed by a dense layer with a single neuron in the output layer. Train the model for 10 epochs with Mean square error (MSE) loss. \n",
        "\n",
        "Try to manually calculate the number of parameters in your model's architecture and compare it with the one reported by `summary()`. Finally, plot the learning curves (validation and training loss vs. epochs) and report the MAE on the test set in the unit of degrees Celsius."
      ],
      "metadata": {
        "id": "qLJkoaPELjzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "-ZVzGaBCXLDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Build an RNN with two layers of GRU with 16 cells in each layer, followed by a dense layer with a single neuron in the output layer. In addition, apply the dropout for the input units of the dense layer. The dropout rate should be set to 0.5. Train the model for 10 epochs with MSE loss. \n",
        "\n",
        "Finally, plot the learning curves and report the MAE on the test set in the unit of degrees Celsius."
      ],
      "metadata": {
        "id": "3d5qD12pL8nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "eFiSKQrpgz-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Build an RNN as in (c). But this time, also apply the dropout to the hidden state at each time step and try to **unroll the RNN during training**. The dropout rate should be set to 0.5. Train the model for **one** epoch with MSE loss. \n",
        "\n",
        "Report the time required to train the network compared with (c) and make some comments on it.\n",
        "\n",
        "Hint: Look up the document https://keras.io/api/layers/recurrent_layers/gru/ to see how to apply dropout and unroll the loop of RNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "sPklQkddhj8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "NipS2aIIMC0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Test classification using the preprocessed IMDB dataset"
      ],
      "metadata": {
        "id": "XHmRCbF1VtRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, we will continue to work with the IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. It consists of 50% negative and 50% positive reviews. **Our job is trying to classify the reviews into positive or negative classes.** We will focus on the model building, training, and evaluation part. Therefore, we will use the preprocessed dataset from Keras.\n",
        "\n",
        "https://ai.stanford.edu/~amaas/data/sentiment/."
      ],
      "metadata": {
        "id": "Tzp0m9WpYTUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Load the IMDB dataset with `keras.datasets.imdb.load_data()` and only keep the top 10,000 most frequently occurring words in the training data. In addition, split it into a training set (25,000 images), a validation set (5,000 images), and a test set (20,000 images). Finally, pad or truncate the input sequence to 500 words.\n",
        "\n",
        "Hint: You may find https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences and https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data helpful."
      ],
      "metadata": {
        "id": "Ngx3M8SStw5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "NaGlp4WvM1zU",
        "execution": {
          "iopub.status.busy": "2022-02-27T13:08:48.116619Z",
          "iopub.execute_input": "2022-02-27T13:08:48.117362Z",
          "iopub.status.idle": "2022-02-27T13:08:49.254055Z",
          "shell.execute_reply.started": "2022-02-27T13:08:48.117311Z",
          "shell.execute_reply": "2022-02-27T13:08:49.253224Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Build a 1D Convolutional Neural Network using the following architecture and remember to mask your input in the embedding layer (Remember that the embedding layer is capable of generating a“mask” that corresponds to its input data as described in the laboratory): \n",
        "\n",
        "\n",
        "|        | Type                | Maps    | Activation | Notice|\n",
        "|--------|---------------------|---------|------------|------------|\n",
        "| Output | Fully connected     |       | Sigmoid    ||\n",
        "| D3     | Dropout         | |  |with dropout rate set to 0.75|\n",
        "| P1     | 1D Max Pooling         |         |            |with pooling size set to 2|\n",
        "| D2     | Dropout |         |            |with dropout rate set to 0.75|\n",
        "| C1     | 1D Convolution         | 32      | ReLu       |with kernel size set to 4, stride set to 2 and apply same padding|\n",
        "| D1     | Dropout |         |            |with dropout rate set to 0.75|\n",
        "| E1     | Embedding         |       |        | Output of embedding is set to 128 dimension|\n",
        "| In     | Input               |  |            |Input is truncated to 500 words with 10,000 dimension|\n",
        "\n",
        "Train the model for 10 epochs with Adam optimizer. Finally, plot the learning curves and report the accuracy on the test set."
      ],
      "metadata": {
        "id": "L3ZRsJXwtw5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "B6Yj6PsB2hp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) There is a [rule of thumb](https://developers.google.com/machine-learning/guides/text-classification/step-2-5#algorithm_for_data_preparation_and_model_building) that you should pay close attention to the **ratio between the number of samples in your training data and the mean number of words per sample** when approaching a new text classification task. If that ratio is smaller or less than 1,500, the bag-of-bigrams model will perform better (and as a bonus, it will be much faster to train and iterate on too). If that ratio is higher than 1,500, you should go with a sequence model. In other words, sequence models work best when lots of training data are available and when each sample is relatively short.\n",
        "\n",
        "Try to plot the Histogram of the number of words per sample for the IMDB training dataset and calculate the ratio described above. Finally, compare the accuracy we get using bag-of-bigrams in the laboratory and the results you get in (b). Make some comments on the rule of thumb."
      ],
      "metadata": {
        "id": "HugAnsOHPKVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "KHccjuHtvuEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: Transfer learning and network architecture search for CIFAR-10 dataset\n"
      ],
      "metadata": {
        "id": "aBDkj0NJ18kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, we will try to boost the classification performance on the CIFAR-10 dataset. The techniques will include transfer learning and network architecture search.\n",
        "\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html."
      ],
      "metadata": {
        "id": "fCntJOu5qJkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Firstly, Load the CIFAR-10 dataset (you may refer to `keras.datasets.cifar10.load_data()`) as in Assignment 1, and split it into a training set (45,000 images), a validation set (5,000 images) and a test set (10,000 images)."
      ],
      "metadata": {
        "id": "XvzpnzpQqqlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "hEclHDblx_7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) EfficientNet is a modern convnets obtained from network architecture search. Use the convolutional base of `efficientnetB0` and pretrained weight from ImageNet. Try to freeze all the variables in the convolutional base. In addition, add a dropout layer with a dropout rate set to 0.5 followed by a dense layer with softmax activation.\n",
        "\n",
        "Train the model for 10 epochs using SGD optimizer with a learning rate of 0.01. Finally, plot the learning curves and report the accuracy on the test set."
      ],
      "metadata": {
        "id": "8fSsEjKA88Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "1On7VNhrQ3_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Use the same architecture as (b), but this time unfreeze all the layers (i.e.  We will fine-tune all the layers!). Train the model for 10 epochs using SGD optimizer with a learning rate of 0.01. Finally, plot the learning curves and report the accuracy on the test set. Compared the results with (a) and made some comments.\n"
      ],
      "metadata": {
        "id": "X1TICQ99_TbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "-Byy3TksRbtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Use Keras Tuner to do the network architecture search. The search space is described as follows:\n",
        "\n",
        "|        | Type                | Activation | Notice|\n",
        "|--------|---------------------|---------|------------|\n",
        "| Output | Fully connected    | Softmax    ||\n",
        "| D1 | DropOut     |        ||\n",
        "| F1     | Fully connected         | ReLu ||\n",
        "| PN     | Global average pooling              |            ||\n",
        "| ...     |  |                  |The convoltion blocks may repeat 3~5 times|\n",
        "| P1     | Max pooling     |------------|\\||\n",
        "| R2     | ReLu         ||\\||\n",
        "| B2     | batch normalization ||\\||\n",
        "| C2     | Convolution     ||\\|-------> These 7 layer forms 1 convolution blocks|\n",
        "| R1     | ReLu         ||\\||\n",
        "| B1     | batch normalization ||\\||\n",
        "| C1     | Convolution      |------------|\\||\n",
        "| In     | Input         |           ||\n",
        "\n",
        "\n",
        "1. Search the number of convolutional blocks (a single block contains seven layers: (convolution, batch normalization, relu)*2 followed by a pooling layer) from 3 to 5.\n",
        "2. Search the number of filters used in convolutional layers in the convolutional blocks from 32 to 256 with step size set to 32\n",
        "3. Search the number of neurons in the first dense layer from 30 to 100 with step size set to 10\n",
        "4. Search the dropout rate from 0 to 0.5 with the step size set to 0.1.\n",
        "5. Use the Adam optimizer and search the learning rate from 0.0001 to 0.01 with sampling strategy set to \"log\".\n",
        "\n",
        "Use Bayesian optimization to search for a maximum of 3 trials with two executions per trial. To speed up the search, **only includes the first 1000 images from the training set** but evaluate the performance on the whole validation set for 10 epochs.\n",
        "\n",
        "Finally, report the architecture you find."
      ],
      "metadata": {
        "id": "aKYcT76QtdHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "HvCd1JahS6vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Train the model you find in (d) for 10 epochs on the full training set.\n",
        "Finally, plot the learning curves and report the accuracy on the test set."
      ],
      "metadata": {
        "id": "Ty5y7QjR_jA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "esi_sZb-xT8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}