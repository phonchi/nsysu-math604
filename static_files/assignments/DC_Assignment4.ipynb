{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "DC_Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4"
      ],
      "metadata": {
        "id": "ZguKQsaEtw45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student ID: *Double click here to fill the Student ID*\n",
        "\n",
        "#### Name: *Double click here to fill the name*"
      ],
      "metadata": {
        "id": "RxvUeukG506E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, install the following dependencies. You may need to restart the kernel after the installation."
      ],
      "metadata": {
        "id": "xX8YwqcZCk8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost -U -qq\n",
        "!pip install xgboost -U -qq\n",
        "!pip install lightgbm -U -qq\n",
        "!pip install shap -qq\n",
        "!pip install imodels -qq\n",
        "!pip install bentoml --pre -qq\n",
        "!pip install pyngrok -qq\n",
        "!pip install PyYAML -U -qq"
      ],
      "metadata": {
        "id": "3wXbOK6XnqFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are using Colab or Kaggle notebook, try to set up the tunnel using the following commands."
      ],
      "metadata": {
        "id": "9KH62SYtC7cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass"
      ],
      "metadata": {
        "id": "BwdfR79uPS3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
        "conf.get_default().auth_token = getpass.getpass()"
      ],
      "metadata": {
        "id": "JXUtSStSPNH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a tunnel to the streamlit port 8050\n",
        "public_url = ngrok.connect(8050)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv6flwNTPPsR",
        "outputId": "f568c08f-96f2-4127-ed09-1e7cdf549654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Analyze hospital readmission dataset with interpretable methods\n",
        "\n",
        "Hospital readmission is an episode when a patient who had been discharged from a hospital is admitted again within a specified time interval. Generally, a higher readmission rate indicates the ineffectiveness of treatment during past hospitalizations. \n",
        "\n",
        "Therefore, the hospital wants your help identifying patients at the highest risk of being readmitted. Doctors will take care of the final decision about when to release each patient, but they hope you could build a model to highlight issues the doctors should consider when discharging a patient. The hospital has given you relevant patient medical information. The given dataset contains the following features:"
      ],
      "metadata": {
        "id": "FZWBn2fiCh0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Your prediction target is `readmitted`\n",
        "* Feature names like `number_inpatient` refers to the number of inpatient visits of the patient in the year preceding the encounter\n",
        "* Features whose names with the word `diag` indicate the diagnostic code of the illness or illnesses the patient was admitted with. For example, `diag_1_428` means the doctor said their first illness diagnosis is number \"428\".  \n",
        "* A feature name like `metformin_No` means the patient did not have the medicine `metformin.` If this feature had a value of False, then the patient did take the drug `metformin.`\n",
        "* Features whose names begin with `medical_specialty` describe the specialty of the doctor who treats the patient. The values in these fields are all `True` or `False.`"
      ],
      "metadata": {
        "id": "hs7Gkw7hjDyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, use the following code snippet to set up the dataset. Note the CSV file can be downloaded from our course website."
      ],
      "metadata": {
        "id": "cjYH5-oJ6Xmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "# Read the data and fix some column names\n",
        "X = pd.read_csv('hospital.csv')\n",
        "X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "# Split into targt and features\n",
        "y = X.readmitted              \n",
        "X.drop(['readmitted'], axis=1, inplace=True)\n",
        "\n",
        "# Split into training and test set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1)"
      ],
      "metadata": {
        "id": "htkJGrOd1N7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following questions, when you use methods\n",
        "that are inherently random, make sure to use, set the random seed to 42."
      ],
      "metadata": {
        "id": "clIL6XHD_pJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Firstly, fit an interpretable model and show doctors some evidence the model is doing something in line with their medical intuition. \n",
        "\n",
        "* Try to build an OneR decision rule model as a baseline and calculate the accuracy of the OneR model on the validation set. \n",
        "* Now, two patients (Their data are recorded in 21492 and 15100 rows in the original dataset) come in, and they would like to know why they get readmitted or do not get readmitted. Use the rule list generated from the OneR model to give them a reason.  "
      ],
      "metadata": {
        "id": "9batOLKbJmLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "MdSa5jVRKDBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) The doctor is glad that you convinced the patients, but he is still worried about the model performance of OneR you just built. \n",
        "\n",
        "* Try to build a more complicated classifier using `LightGBM`, train the model that gives a maximum of 5,000 trees and will stop after 100 consecutive rounds fail to find any improvement. \n",
        "* Then report the accuracy of the validation set. Finally, plot the feature importance (Use `gain` as the type of importance instead of `split`) of the top ten important features.\n",
        "\n",
        "Hint: The [early stop callback](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.early_stopping.html) and [feature_importance](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster.feature_importance) attributes may be useful."
      ],
      "metadata": {
        "id": "aP7WbFiVNy0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "kLEUHx5BpTbO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) It appears `number_inpatient` is a critical feature, and the doctors would like to know more about that. \n",
        "\n",
        "* Create a partial dependence plot and an individual conditional expectation plot for them that shows how `num_inpatient` affects the model's predictions. (You should use the validation set to generate the plot)\n",
        "* In addition, also create the partial dependence plot and the individual conditional expectation plot for the `time_in_hospital` so that they can tell from these plots whether the effect of the `number_inpatient` on the target is big or small. \n",
        "* Comment on your results."
      ],
      "metadata": {
        "id": "qLJkoaPELjzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "5ZgGtHI19-R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Now, the doctors are looking for the local explanations of your model. \n",
        "* Try to create a force plot (for the previous two patients) and a summary plot of Shapley values.  (You should use the validation set to generate the plot)\n",
        "* Use the force plot to give the previous two patients (Their data are recorded in 21492 and 15100 rows in the original dataset) why they were readmitted or not.\n",
        "* Does the summary plot consistent with the feature importance plot?"
      ],
      "metadata": {
        "id": "3d5qD12pL8nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "Ma3pYM2wF9z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) Now the doctors are convinced you have the right data, and the model overview looked reasonable. It's time to turn this into a finished product they can use. \n",
        "* Try to use `BentoML` to save your LightGBM model, then reload it. \n",
        "* Use the reloaded model to make inferences on the previous two patients. Show that the inference results are the same as the original model for these two patients.\n",
        "\n",
        "Hint: You may refer to https://docs.bentoml.org/en/latest/frameworks/lightgbm.html and our lab for more information."
      ],
      "metadata": {
        "id": "otDoP1JUL-Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "I75kHrK9RBMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(f) Deploy your model as a REST API server using `BentoML.` In addition, test your server by sending a request that contains the data of the previous two patients. Show that the responses from the server are the same as the predictions of the original model for these two patients.\n",
        "\n",
        "Hint: You should first transform the boolean value to an integer (True to 1 and False to 0) and transform the integer to a string before sending the request to your server."
      ],
      "metadata": {
        "id": "stow9mhWO1HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "07O340juQj4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2 Revisit the Ames housing dataset with ensemble learning"
      ],
      "metadata": {
        "id": "XRcidX-3dG-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will revisit the Ames Housing dataset from assignment 3 in this question. However, we would like to improve our performance using ensemble learning this time."
      ],
      "metadata": {
        "id": "fNGW83jN-8rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, use the following code snippet to set up the dataset. Note the CSV file can be downloaded from our course website."
      ],
      "metadata": {
        "id": "sQEDAWhMAw0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "X = pd.read_csv(\"ames.csv\")\n",
        "\n",
        "# Separate target from predictors\n",
        "y = X.SalePrice              \n",
        "X.drop(['SalePrice'], axis=1, inplace=True)\n",
        "\n",
        "# Break off validation set from training data\n",
        "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)\n",
        "\n",
        "# \"Cardinality\" means the number of unique values in a column\n",
        "# Here, we select categorical columns with relatively low cardinality \n",
        "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
        "                        X_train_full[cname].dtype == \"object\"]\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Keep selected columns only\n",
        "my_cols = low_cardinality_cols + numeric_cols\n",
        "X_train = X_train_full[my_cols].copy()\n",
        "X_valid = X_valid_full[my_cols].copy()"
      ],
      "metadata": {
        "id": "9XPA7wJSdWtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following questions, when you use methods\n",
        "that are inherently random, make sure to use, set the random seed to 42."
      ],
      "metadata": {
        "id": "ni_SrGDeA1Nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) You'll build and train a gradient boosting in this step.\n",
        "\n",
        "* Use one-hot encoding for the categorical variable.\n",
        "* Training an XGBoost regression model.   **Leave all other parameters as default.**\n",
        "* Then, fit the model to the training data and calculate the mean absolute error (MAE) corresponding to the predictions for the validation set.\n",
        "\n",
        "Hint: Since we are working with both training and validation sets, try to apply the same transform when you encode the variables. You may find [ColumnTransformer](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) and the `handle_unknown` option in the encoder useful."
      ],
      "metadata": {
        "id": "RXbQ2Pt6_Rcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "KBHaU8XCBgQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) A simple technique for improving the gradient boosting model is to use a lower learning rate with larger iterations. Try to build another `XGBRegressor` with a learning rate set to 0.05 and the number of estimators set to 1000 and calculate the MAE on the validation set."
      ],
      "metadata": {
        "id": "cb-KMcC0jQM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "gHZYi1GxtrbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Now train an LGBMRegressor and a CatBoostRegressor with default parameters on the original training data (do not use one-hot encoding before feeding into the models this time; instead, let the models deal with it). Calculate the MAE on the validation set for these two models.\n",
        "\n",
        "Hint: You may refer to our lab to see how to deal with categorical variables inside these two frameworks."
      ],
      "metadata": {
        "id": "UX_ZYkAbfHLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "0S88i0V9lhQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Build an ensemble model using the hard voting regressor for the `XGBRegressor` in (c), the `LGBMRegressor`, and the `CatBoostRegressor` in (d). Fit the ensemble model on the original training dataset and calculate the MAE on the validation set. In addition, also build a stacking regressor for these three models, fit on the original dataset and calculate the MAE on the validation set. Finally, comment on your results.\n",
        "\n",
        "Hint: You may find [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) useful when you combine three models with different preprocessing steps."
      ],
      "metadata": {
        "id": "h_cM8zt0lmtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "9Ddm5ra5wQxD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}