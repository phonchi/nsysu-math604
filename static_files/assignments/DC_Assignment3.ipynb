{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "DC_Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3"
      ],
      "metadata": {
        "id": "ZguKQsaEtw45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Student ID: *Double click here to fill the Student ID*\n",
        "\n",
        "#### Name: *Double click here to fill the name*"
      ],
      "metadata": {
        "id": "RxvUeukG506E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1: Analyze StackOverflow dataset using SQL"
      ],
      "metadata": {
        "id": "FZWBn2fiCh0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle has a rich number of [BigQuery](https://www.kaggle.com/datasets?fileType=bigQuery) and [SQLite](https://www.kaggle.com/datasets?fileType=sqlite) datasets that you can practice your SQL skill.\n",
        "\n",
        "In this question, we are going to examine the StackOverflow dataset. [Stack Overflow](https://stackoverflow.com/) is a popular question and answer site for technical questions. We will explore how to set up a service that identifies the Stack Overflow users who have demonstrated expertise within a specific field by answering related questions about it.\n",
        "\n",
        "Hint: It is recommended to answer this question in Kaggle, where you can access the dataset directly. In addition, refer to https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf and our laboratory for useful commands."
      ],
      "metadata": {
        "id": "BSjqQS6kyafv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, if you are using colab, use the following code snippet to setup the client. For more detail, please refer to our lab."
      ],
      "metadata": {
        "id": "w9iXYvI7gHDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# Replace the following variable your key path \n",
        "key_path = \"utopian-datum-340514-9ffc23108bf4.json\"\n",
        "\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
        ")\n",
        "\n",
        "client = bigquery.Client(credentials=credentials, project=credentials.project_id,)"
      ],
      "metadata": {
        "id": "_ArDmIuhJpdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Try to print the names of all tables available in the dataset first. \n",
        "Then, try to print the table schema of the `posts_answers` table and the `posts_questions` tables. Finally, preview the first ten rows of the above two tables by retrieving the data frame.\n",
        "\n",
        "Hint: the dataset can be access via `dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")`"
      ],
      "metadata": {
        "id": "9batOLKbJmLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "l5Q6xHq6ggOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) `posts_questions` has a column called tags which lists the topics/technologies each question is about. Please write a query that selects the `id`, `title`, `parent_id` and `owner_display_name` columns from the `posts_questions` table and follow the below restrictions. \n",
        "\n",
        "* Restrict the results to rows that contain the word 'sql' in the `tags` column. (Remember the `SELET...WHERE` clause. Do not use wildcard here.)\n",
        "* Rename the column name `title` to `question_title` (Remember the `AS` clause)\n",
        "* Sort the results by the `score` column in descending order (Remember the `ORDER BY` clause)\n",
        "\n",
        "In the retrieve data frame, how many rows do you have? How many rows have non-null `ower_dispaly_name`? (Which means the `owner_display_name` is not 'None')."
      ],
      "metadata": {
        "id": "qLJkoaPELjzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here.\n",
        "query = \"\"\"\n",
        "                  \"\"\"\n",
        "\n",
        "############   Do not modify the code below     ############ \n",
        "# Set up the query (cancel the query if it would use too much of \n",
        "# your quota, with the limit set to 1 GB)\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
        "query_job = client.query(query, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and return a pandas DataFrame\n",
        "results = query_job.to_dataframe()\n",
        "############   Do not modify the code above     ############ "
      ],
      "metadata": {
        "id": "Damp37PTl-Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) `posts_answers` has a column called `parent_id` which identifies the ID of the question each answer corresponds to. You can then join two tables by the `parent_id` in `posts_answers` and the `id` in `posts_questions`. `posts_answers` also has an `owner_user_id` column which specifies the ID of the user who answered the question. \n",
        "\n",
        "Now, write a query that has a single row for each user who answered at least one question with a tag that equals to string `sql`. Your results should have two columns (Remember the `SELET...WHERE` clause) and follow the below restrictions:\n",
        "\n",
        "* `owner_user_id` - contains the `owner_user_id` column from the `posts_answers` table (Remember the `INNER JOIN` clause)\n",
        "* `number_of_answers` - contains the number of answers the user has written to `sql` related questions (Remember the `AS`, `GROUP BY` and `COUNT` clause)\n",
        "* Filter out the rows whose `number_of_answers` is smaller or equal to 2 (Remember the `HAVING` clause)\n",
        "\n",
        "In the retrieve data frame, which user answers most questions in the sql domain? The user is an expert in the SQL domain and is the author of many SQL books, try to find the name of a book the user wrote.\n",
        "\n",
        "Hint: You can find the public user profile by appending the ID after https://stackoverflow.com/users/"
      ],
      "metadata": {
        "id": "3d5qD12pL8nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here.\n",
        "query = \"\"\"\n",
        "                         \"\"\"\n",
        "\n",
        "############   Do not modify the code below     ############ \n",
        "# Set up the query (cancel the query if it would use too much of \n",
        "# your quota, with the limit set to 1 GB)\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
        "query_job = client.query(query, job_config=safe_config)\n",
        "\n",
        "# API request - run the query, and return a pandas DataFrame\n",
        "results = query_job.to_dataframe()\n",
        "############   Do not modify the code above     ############ "
      ],
      "metadata": {
        "id": "NipS2aIIMC0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Data cleaning with Melbourne Housing dataset"
      ],
      "metadata": {
        "id": "XHmRCbF1VtRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, you are going to practice the data preparation skill that is often used in a real-world project.\n",
        "\n",
        "The dataset is a snapshot of a [dataset](https://www.kaggle.com/datasets/anthonypino/melbourne-housing-market) created by Tony Pino. It was scraped from publicly available results posted every week from https://www.domain.com.au/. The dataset includes Address, Type of Real estate, Suburb, Method of Selling, Rooms, Price, Real Estate Agent, Date of Sale and distance from the central business district, etc."
      ],
      "metadata": {
        "id": "Tzp0m9WpYTUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, use the following code snippet to set up the dataset. Note the CSV file can be downloaded from our course website."
      ],
      "metadata": {
        "id": "iAlaL-SqlnUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Function to calculate mae by fixing the model\n",
        "def scoring_mae(X_train, X_valid, y_train, y_valid):\n",
        "    model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_valid)\n",
        "    return mean_absolute_error(y_valid, preds)\n",
        "\n",
        "data = pd.read_csv('melb_data.csv')\n",
        "y = data.Price\n",
        "\n",
        "# To keep things simple, we'll split the columns into numerical can categorical features\n",
        "melb_predictors = data.drop(['Price', 'Date', 'Address'], axis=1)\n",
        "cat_col = melb_predictors.select_dtypes(exclude=['int64','float64'])\n",
        "\n",
        "# Divide data into training and validation subsets, try to use the resulting datafram below in the following questions\n",
        "X, X_v, y_train, y_valid = train_test_split(melb_predictors, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "# Numerical columns\n",
        "X_train = X.select_dtypes(exclude=['object'])\n",
        "X_valid = X_v.select_dtypes(exclude=['object'])\n",
        "# Categorical columns\n",
        "X_train_cat = X.select_dtypes(exclude=['int64','float64'])\n",
        "X_valid_cat = X_v.select_dtypes(exclude=['int64','float64'])"
      ],
      "metadata": {
        "id": "jnYu43u87Kpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Firstly, try to find out which columns contain missing values. Then, try to calculate the percentage of the missing values in the dataset.  \n",
        "\n",
        "Hint: You should count the missing rate base on the original `data` matrix"
      ],
      "metadata": {
        "id": "Ngx3M8SStw5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "RkCalb5vnJ_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Considering only the numerical columns in this question. We are going to compare the MAE between the three data cleaning approach.\n",
        "\n",
        "1. Removing all the columns with empty values. \n",
        "2. Replace missing values with the median value along each column.\n",
        "3. Use the iterative imputation method and set the regressor to KNN regressor with 20 neighbors.\n",
        "\n",
        "Use the `scoring_mae` function to perform regression and calculate the MAE for each approach. Finally, make some comments on the results.\n",
        "\n",
        "Hint: Since we are working with both training and validation sets, try to drop the same columns in both data frames. In addition, you should apply the same transform when you impute the missing value."
      ],
      "metadata": {
        "id": "L3ZRsJXwtw5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "rWDX1pGn_-lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Considering only the categorical columns in this question. Find out which columns contain missing values first. Then, try to \n",
        "\n",
        "* Replace missing values with the most frequent value along each column. \n",
        "* Perform one-hot encoding for the categorical variables whose number of categories is smaller than 10; otherwise, use the label encoding for that column. \n",
        "* When there are unknown categories in the validation set, set it to all zeros for the one-hot encoding and set it to -1 for the label encoding.\n",
        "\n",
        "Use the `scoring_mae` function to perform regression and calculate the MAE for the resulting data.\n",
        "\n",
        "Hint: Since we are working with both training and validation sets, try to apply the same transform when you impute the missing data or encode the variables. You may find [ColumnTransformer](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) and the `handle_unknown` option in the encoder useful."
      ],
      "metadata": {
        "id": "HugAnsOHPKVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "oOq55bOkqDY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Combining the transformation in (b) and (c) to the original training and validation data split (`X` and `X_v`) and using the `scoring_mae` function to calculate the MAE. Make some comments on the results when comparing with (b) and (c).\n",
        "\n",
        "Hint: Since we are working with both training and validation sets, try to apply the same transform when you impute the missing data or encode the variables. You may find [ColumnTransformer](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) useful."
      ],
      "metadata": {
        "id": "k9U3qjndOyRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "v2eRb0YY_HtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: Feature engineering and selection with Ames housing dataset"
      ],
      "metadata": {
        "id": "aBDkj0NJ18kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, we are going to examine several feature engineering and feature selection methods.\n",
        "\n",
        "The dataset we are going to use is a modified version of the Ames housing dataset. The original data was compiled by Dean De Cock for use in data science education and published in [De Cock, D. (2011)](https://www.tandfonline.com/doi/abs/10.1080/10691898.2011.11889627). The modified version contains 2930 rows with 79 columns describing every aspect of residential homes in Ames, Iowa.  "
      ],
      "metadata": {
        "id": "fCntJOu5qJkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, use the following code snippet to set up the dataset. Note the CSV file can be downloaded from our course website."
      ],
      "metadata": {
        "id": "28vMkQdMuD8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Notice that the categorical variables in your dataset will be label encoded \n",
        "# and change the original value when you call the following functions \n",
        "def scoring_rmsle(X, y, model=RandomForestRegressor()):\n",
        "    # Label encoding for categorical variables\n",
        "    for colname in X.select_dtypes([\"category\", \"object\"]):\n",
        "        X[colname], _ = X[colname].factorize()\n",
        "    # Metric is RMSLE (Root Mean Squared Log Error)\n",
        "    score = cross_val_score(\n",
        "        model, X, y, cv=5, scoring=\"neg_mean_squared_log_error\",\n",
        "    )\n",
        "    score = -1 * score.mean()\n",
        "    score = np.sqrt(score)\n",
        "    return score\n",
        "\n",
        "# Prepare data\n",
        "df = pd.read_csv(\"ames.csv\")\n",
        "X = df.copy()\n",
        "y = X.pop(\"SalePrice\")"
      ],
      "metadata": {
        "id": "CuXdXPyheslS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) First try to perform the k-means clustering with the following parameters:\n",
        "* features: `FirstFlrSF`, `SecondFlrSF`,`GrLivArea`, `LotArea`, `TotalBsmtSF` \n",
        "* number of clusters: 10\n",
        "\n",
        "Then, add the k-means label and cluster-distances features to your original dataset. Finally, perform the regression and calculate the RMLSE using the `scoring_rmsle` function.\n",
        "\n",
        "Hint: You may find the `predict` and `transform` in the [k-means function](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) useful when you are generating the features. "
      ],
      "metadata": {
        "id": "XvzpnzpQqqlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "hEclHDblx_7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Try to perform the permutation importance with the regressor set to extra-trees. Draw the bar plot of the feature importance of the top 10 most important features. Finally, use the most important 50 features to perform regression and calculate the RMLSE using the `scoring_rmsle` function.\n",
        "\n",
        "Hint: The categorical variables should be label encoded when calculating permutation importance."
      ],
      "metadata": {
        "id": "8fSsEjKA88Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "kakHo82CfadZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Use the [Boruta](https://pdfs.semanticscholar.org/ecc2/ca3150dc4d4d8dceedab244114f191e05742.pdf) to perform feature selection. How many features does Boruta select in this example? Then, use the selected features to perform regression and calculate the RMLSE using the `scoring_rmsle` function. Finally, make some comments comparing with (a) and (b).\n",
        "\n",
        "Hint: It may take some time to perform the Boruta. The categorical variables should be label encoded when using Boruta."
      ],
      "metadata": {
        "id": "X1TICQ99_TbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding your answer here."
      ],
      "metadata": {
        "id": "7Co4ByfPqNpW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}