{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Chapter 17 – Autoencoders and GANs**","metadata":{"id":"p8OEWQEkn2xq"}},{"cell_type":"markdown","source":"_This notebook contains all the sample code in chapter 17._","metadata":{"id":"UFP_mZlGn2xs"}},{"cell_type":"markdown","source":"<table align=\"left\">\n  <td>\n    <a href=\"https://colab.research.google.com/github/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n  </td>\n</table>","metadata":{"id":"Ba_pO0hyn2xt"}},{"cell_type":"markdown","source":"# Setup","metadata":{"id":"QNf_VR7fn2xt"}},{"cell_type":"markdown","source":"First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.","metadata":{"id":"G-lDmrksn2xt"}},{"cell_type":"code","source":"# Python ≥3.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Is this notebook running on Colab or Kaggle?\nIS_COLAB = \"google.colab\" in sys.modules\nIS_KAGGLE = \"kaggle_secrets\" in sys.modules\n\n# Scikit-Learn ≥0.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# TensorFlow ≥2.0 is required\nimport tensorflow as tf\nfrom tensorflow import keras\nassert tf.__version__ >= \"2.0\"\n\nif not tf.config.list_physical_devices('GPU'):\n    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n    if IS_COLAB:\n        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n    if IS_KAGGLE:\n        print(\"Go to Settings > Accelerator and select GPU.\")\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"autoencoders\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)","metadata":{"id":"vBYjWezcn2xu","execution":{"iopub.status.busy":"2022-03-11T06:29:44.213765Z","iopub.execute_input":"2022-03-11T06:29:44.214043Z","iopub.status.idle":"2022-03-11T06:29:48.908610Z","shell.execute_reply.started":"2022-03-11T06:29:44.214015Z","shell.execute_reply":"2022-03-11T06:29:48.906895Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"A couple utility functions to plot grayscale 28x28 image:","metadata":{"id":"DywhuFAZn2xw"}},{"cell_type":"code","source":"def plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\ndef rounded_accuracy(y_true, y_pred):\n    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n\n\ndef plot_image(image):\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n    \n# Visualize 2d manifold from  encodings using tSNE\n\nfrom sklearn.manifold import TSNE\nimport matplotlib\n\ndef plot_embeddings_tsne(X_data, y_data, encodings):\n  np.random.seed(42)\n  tsne = TSNE()\n  X_data_2D = tsne.fit_transform(encodings)\n  X_data_2D = (X_data_2D - X_data_2D.min()) / (X_data_2D.max() - X_data_2D.min())\n\n  # adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n  plt.figure(figsize=(10, 8))\n  cmap = plt.cm.tab10\n  plt.scatter(X_data_2D[:, 0], X_data_2D[:, 1], c=y_data, s=10, cmap=cmap)\n  image_positions = np.array([[1., 1.]])\n  for index, position in enumerate(X_data_2D):\n      dist = np.sum((position - image_positions) ** 2, axis=1)\n      if np.min(dist) > 0.02: # if far enough from other images\n          image_positions = np.r_[image_positions, [position]]\n          imagebox = matplotlib.offsetbox.AnnotationBbox(\n              matplotlib.offsetbox.OffsetImage(X_data[index], cmap=\"binary\"),\n              position, bboxprops={\"edgecolor\": cmap(y_data[index]), \"lw\": 2})\n          plt.gca().add_artist(imagebox)\n  plt.axis(\"off\") \n\ndef plot_multiple_images(images, n_cols=None):\n    n_cols = n_cols or len(images)\n    n_rows = (len(images) - 1) // n_cols + 1\n    if images.shape[-1] == 1:\n        images = np.squeeze(images, axis=-1)\n    plt.figure(figsize=(n_cols, n_rows))\n    for index, image in enumerate(images):\n        plt.subplot(n_rows, n_cols, index + 1)\n        #plt.imshow(image)\n        plt.imshow(image.astype(\"int32\"), cmap=\"binary\")\n        plt.axis(\"off\")      ","metadata":{"id":"HPkWtE5bn2xw","execution":{"iopub.status.busy":"2022-03-11T07:01:03.206142Z","iopub.execute_input":"2022-03-11T07:01:03.206855Z","iopub.status.idle":"2022-03-11T07:01:03.219548Z","shell.execute_reply.started":"2022-03-11T07:01:03.206807Z","shell.execute_reply":"2022-03-11T07:01:03.218810Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# PCA with a linear Autoencoder","metadata":{"id":"DlbyEZ6En2xx"}},{"cell_type":"markdown","source":"If the autoencoder uses only linear activations and the cost function is the mean squared error (MSE), then it ends up performing Principal Component Analysis. The following code builds a simple linear autoencoder to perform PCA on a 3D dataset, projecting it to 2D","metadata":{"id":"H-fwwczSn2xx"}},{"cell_type":"code","source":"np.random.seed(4)\n\ndef generate_3d_data(m, w1=0.1, w2=0.3, noise=0.1):\n    angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n    data = np.empty((m, 3))\n    data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n    data[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n    data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * np.random.randn(m)\n    return data\n\nX_train = generate_3d_data(60)\nX_train = X_train - X_train.mean(axis=0, keepdims=0)","metadata":{"id":"qtLmo7mLn2xy","execution":{"iopub.status.busy":"2022-03-11T06:30:30.092944Z","iopub.execute_input":"2022-03-11T06:30:30.093539Z","iopub.status.idle":"2022-03-11T06:30:30.103638Z","shell.execute_reply.started":"2022-03-11T06:30:30.093498Z","shell.execute_reply":"2022-03-11T06:30:30.102694Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(6, 3.8))\nax = fig.add_subplot(111, projection='3d')\n\nax.plot(X_train[:, 0], X_train[:, 1], X_train[:, 2], \"k.\")\nax.set_xlabel(\"$x_1$\", fontsize=18, labelpad=10)\nax.set_ylabel(\"$x_2$\", fontsize=18, labelpad=10)\nax.set_zlabel(\"$x_3$\", fontsize=18, labelpad=10)","metadata":{"id":"wm7NmKJ7zpoA","outputId":"64bb6db4-e404-4609-fcca-f88946eb3346","execution":{"iopub.status.busy":"2022-03-11T06:30:30.286282Z","iopub.execute_input":"2022-03-11T06:30:30.286584Z","iopub.status.idle":"2022-03-11T06:30:30.708882Z","shell.execute_reply.started":"2022-03-11T06:30:30.286545Z","shell.execute_reply":"2022-03-11T06:30:30.708259Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Now let's build the Autoencoder...","metadata":{"id":"FnZOvJWcn2xy"}},{"cell_type":"code","source":"np.random.seed(42)\ntf.random.set_seed(42)\n\nencoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\ndecoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\nautoencoder = keras.models.Sequential([encoder, decoder])\n\nautoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1.5))","metadata":{"id":"g0Zlz2Kfn2xy","execution":{"iopub.status.busy":"2022-03-11T06:30:30.909274Z","iopub.execute_input":"2022-03-11T06:30:30.909648Z","iopub.status.idle":"2022-03-11T06:30:34.166157Z","shell.execute_reply.started":"2022-03-11T06:30:30.909616Z","shell.execute_reply":"2022-03-11T06:30:34.164355Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"history = autoencoder.fit(X_train, X_train, epochs=20)","metadata":{"id":"lg8884Wgn2xz","outputId":"1da9f5ce-898b-46d7-d13a-d1bdb341a3bc","execution":{"iopub.status.busy":"2022-03-11T06:30:34.167887Z","iopub.execute_input":"2022-03-11T06:30:34.168119Z","iopub.status.idle":"2022-03-11T06:30:35.486476Z","shell.execute_reply.started":"2022-03-11T06:30:34.168086Z","shell.execute_reply":"2022-03-11T06:30:35.485746Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"codings = encoder.predict(X_train)","metadata":{"id":"buIO2RTan2xz","execution":{"iopub.status.busy":"2022-03-11T06:30:35.488066Z","iopub.execute_input":"2022-03-11T06:30:35.488337Z","iopub.status.idle":"2022-03-11T06:30:35.588105Z","shell.execute_reply.started":"2022-03-11T06:30:35.488288Z","shell.execute_reply":"2022-03-11T06:30:35.587449Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(4,3))\nplt.plot(codings[:,0], codings[:, 1], \"b.\")\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\nplt.grid(True)\nsave_fig(\"linear_autoencoder_pca_plot\")\nplt.show()","metadata":{"id":"zPzL9vqXn2xz","outputId":"5f6d932b-c0a5-4a67-b57b-32476ff8fe61","execution":{"iopub.status.busy":"2022-03-11T06:30:35.590007Z","iopub.execute_input":"2022-03-11T06:30:35.590247Z","iopub.status.idle":"2022-03-11T06:30:35.979782Z","shell.execute_reply.started":"2022-03-11T06:30:35.590212Z","shell.execute_reply":"2022-03-11T06:30:35.979111Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the autoencoder found the best 2D plane to project the data onto, preserving as much variance in the data as it could (just like PCA).","metadata":{"id":"9ZyBd_fnyVef"}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ncodings2 = pca.fit_transform(X_train)","metadata":{"id":"NQUG1_aQy7sv","execution":{"iopub.status.busy":"2022-03-11T06:30:35.981153Z","iopub.execute_input":"2022-03-11T06:30:35.981610Z","iopub.status.idle":"2022-03-11T06:30:35.998413Z","shell.execute_reply.started":"2022-03-11T06:30:35.981573Z","shell.execute_reply":"2022-03-11T06:30:35.997360Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(4,3))\nplt.plot(codings2[:,0], codings2[:, 1], \"b.\")\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\nplt.grid(True)\nplt.show()","metadata":{"id":"BgF0scIQzGb3","outputId":"20e43d7a-a80f-4c77-c07c-71a6ad58d27f","execution":{"iopub.status.busy":"2022-03-11T06:30:35.999854Z","iopub.execute_input":"2022-03-11T06:30:36.000126Z","iopub.status.idle":"2022-03-11T06:30:36.189139Z","shell.execute_reply.started":"2022-03-11T06:30:36.000089Z","shell.execute_reply":"2022-03-11T06:30:36.188362Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Stacked Autoencoders","metadata":{"id":"-7tZP0J8n2xz"}},{"cell_type":"code","source":"(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\nX_train_full = X_train_full.astype(np.float32) / 255\nX_test = X_test.astype(np.float32) / 255\nX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\ny_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]","metadata":{"id":"6s0Rmqrhn2x0","outputId":"48459054-15b1-41f4-d548-40bb7bffe682","execution":{"iopub.status.busy":"2022-03-11T06:30:36.191051Z","iopub.execute_input":"2022-03-11T06:30:36.191811Z","iopub.status.idle":"2022-03-11T06:30:37.270030Z","shell.execute_reply.started":"2022-03-11T06:30:36.191728Z","shell.execute_reply":"2022-03-11T06:30:37.269269Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## MLP Layer","metadata":{"id":"zw34G9Yan2x0"}},{"cell_type":"markdown","source":"You can implement a stacked autoencoder very much like a regular deep MLP. The following code builds a stacked autoencoder for Fashion MNIST. we split the autoencoder model into two submodels: the encoder and the decoder.\n\n* The encoder takes 28 × 28–pixel grayscale images, flattens them so that each image is represented as a vector of size 784, then processes these vectors through two Dense layers of diminishing sizes (100 units then 30 units), both using the SELU activation function. For each input image, the encoder outputs a vector of size 30.\n* The decoder takes codings of size 30 (output by the encoder) and processes them through two Dense layers of increasing sizes (100 units then 784 units), and it reshapes the final vectors into 28 × 28 arrays so the decoder’s outputs have the same shape as the encoder’s inputs.\n* When compiling the stacked autoencoder, we use the binary cross-entropy loss instead of the mean squared error. **We are treating the reconstruction task as a multilabel binary classification problem: each pixel intensity represents the probability that the pixel should be black**. Framing it this way (rather than\nas a regression problem) tends to make the model converge faster\n* We train the model using `X_train` as **both the inputs and the targets** (and similarly, we use `X_valid` as both the validation inputs and targets)\n","metadata":{"id":"Se_iPHiNn2x0"}},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n\nstacked_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(30, activation=\"selu\"),\n])\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\nstacked_ae.compile(loss=\"binary_crossentropy\",\n                   optimizer=keras.optimizers.SGD(learning_rate=1.5), metrics=[rounded_accuracy])\nhistory = stacked_ae.fit(X_train, X_train, epochs=5,\n                         validation_data=(X_valid, X_valid))","metadata":{"id":"0i_mAWPIn2x2","outputId":"ad6f6f6d-dfd8-48b0-a4b8-30ce093f5f64","execution":{"iopub.status.busy":"2022-03-11T06:30:39.404031Z","iopub.execute_input":"2022-03-11T06:30:39.404282Z","iopub.status.idle":"2022-03-11T06:31:02.982755Z","shell.execute_reply.started":"2022-03-11T06:30:39.404253Z","shell.execute_reply":"2022-03-11T06:31:02.981939Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the Reconstructions","metadata":{"id":"YA4jjyMm2gl4"}},{"cell_type":"markdown","source":"One way to ensure that an autoencoder is properly trained is to compare the inputs and the outputs: the differences should not be too significant. Let’s plot a few images from the validation set, as well as their reconstructions:","metadata":{"id":"7GtBSfEan2x2"}},{"cell_type":"code","source":"def show_reconstructions(model, images=X_valid, n_images=5):\n    reconstructions = model.predict(images[:n_images])\n    plt.figure(figsize=(n_images * 1.5, 3))\n    for image_index in range(n_images):\n        plt.subplot(2, n_images, 1 + image_index)\n        plot_image(images[image_index])\n        plt.subplot(2, n_images, 1 + n_images + image_index)\n        plot_image(reconstructions[image_index])\n        \nshow_reconstructions(stacked_ae)\nsave_fig(\"reconstruction_plot\")","metadata":{"id":"8-9MOcdQn2x3","outputId":"3973e0e0-8d8d-4417-b0df-a274e697c3de","execution":{"iopub.status.busy":"2022-03-11T06:31:02.984428Z","iopub.execute_input":"2022-03-11T06:31:02.985114Z","iopub.status.idle":"2022-03-11T06:31:03.713545Z","shell.execute_reply.started":"2022-03-11T06:31:02.985075Z","shell.execute_reply":"2022-03-11T06:31:03.712694Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Fashion MNIST","metadata":{"id":"K7DhxCCPn2x3"}},{"cell_type":"markdown","source":"Now that we have trained a stacked autoencoder, we can use it to reduce the dataset’s dimensionality. For visualization, this does not give great results compared to other dimensionality reduction algorithms, but one big advantage of autoencoders is that they can handle large datasets, with many instances and many features. **So one strategy is to use an autoencoder to reduce the dimensionality down to a reasonable level, then use another dimensionality reduction algorithm for visualization**. Let’s use this strategy to visualize Fashion MNIST. First, we use the encoder from our stacked autoencoder to reduce the dimensionality down to 30, then we use Scikit-Learn’s implementation of the t-SNE algorithm to reduce the dimensionality down to 2 for visualization:","metadata":{"id":"SbyDaeUd3c9y"}},{"cell_type":"code","source":"Z = stacked_encoder.predict(X_valid)\nprint(Z.shape)\nplot_embeddings_tsne(X_valid, y_valid, Z)\nplt.tight_layout()\nplt.savefig('ae-mlp-fashion-tsne.pdf')\nplt.show()","metadata":{"id":"pEVhcqYVn2x3","outputId":"29e98f26-5be3-4fb6-c1e8-44f78a45ea2b","execution":{"iopub.status.busy":"2022-03-11T06:31:22.004487Z","iopub.execute_input":"2022-03-11T06:31:22.005037Z","iopub.status.idle":"2022-03-11T06:31:56.860700Z","shell.execute_reply.started":"2022-03-11T06:31:22.005000Z","shell.execute_reply":"2022-03-11T06:31:56.859050Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The t-SNE algorithm identified several clusters which match the classes reasonably well (each class is represented with a different color).","metadata":{"id":"rvLQrIz74STx"}},{"cell_type":"markdown","source":"## Using Convolutional Layers Instead of Dense Layers","metadata":{"id":"O2Uj-cZyn2x6"}},{"cell_type":"markdown","source":"If you are dealing with images, then the autoencoders we have seen so far will not work well (unless the images are very small): convolutional neural networks are far better suited than dense networks to work with images. So if you want to build an autoencoder for images, you will need to build a convolutional autoencoder. **The encoder is a regular CNN composed of convolutional layers and pooling layers.** It typically reduces the spatial dimensionality of the inputs (i.e., height and width) while increasing the depth (i.e., the number of feature maps). **The decoder must do the reverse (upscale the image and reduce its depth back to the original dimensions), and for this you can use transpose convolutional layers** (alternatively, you could combine upsampling layers with convolutional layers).","metadata":{"id":"WYrBkg0n4fNT"}},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n\nconv_encoder = keras.models.Sequential([\n    keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),\n    keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.MaxPool2D(pool_size=2),\n    keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.MaxPool2D(pool_size=2)\n])\nconv_decoder = keras.models.Sequential([\n    keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"VALID\", activation=\"selu\",\n                                 input_shape=[3, 3, 64]),\n    keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n\nconv_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.0),\n                metrics=[rounded_accuracy])\nhistory = conv_ae.fit(X_train, X_train, epochs=5,\n                      validation_data=(X_valid, X_valid))","metadata":{"id":"GxNElbhP2KjJ","outputId":"8d2edff7-c122-4bfb-e4fd-10031fbdcb4a","execution":{"iopub.status.busy":"2022-03-11T06:31:56.862112Z","iopub.execute_input":"2022-03-11T06:31:56.862423Z","iopub.status.idle":"2022-03-11T06:32:41.685035Z","shell.execute_reply.started":"2022-03-11T06:31:56.862385Z","shell.execute_reply":"2022-03-11T06:32:41.684352Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"show_reconstructions(conv_ae)","metadata":{"id":"ex-ybeCr4GJ6","outputId":"3eb758d6-0117-4732-872d-9d1aa2c8601f","execution":{"iopub.status.busy":"2022-03-11T06:32:41.686595Z","iopub.execute_input":"2022-03-11T06:32:41.686847Z","iopub.status.idle":"2022-03-11T06:32:42.206027Z","shell.execute_reply.started":"2022-03-11T06:32:41.686812Z","shell.execute_reply":"2022-03-11T06:32:42.205361Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"Z = conv_encoder.predict(X_valid)\nprint(Z.shape)\nN = Z.shape[0]\nZZ = np.reshape(Z, (N,-1))\nprint(ZZ.shape)\n\n\nplot_embeddings_tsne(X_valid, y_valid, ZZ)\nplt.tight_layout()\nplt.savefig('ae-conv-fashion-tsne.pdf')\nplt.show()","metadata":{"id":"MgTb3EQhn2x6","outputId":"bc48c83f-e983-403c-efc6-2ddb4839afd0","execution":{"iopub.status.busy":"2022-03-11T06:32:42.207921Z","iopub.execute_input":"2022-03-11T06:32:42.208652Z","iopub.status.idle":"2022-03-11T06:33:18.535740Z","shell.execute_reply.started":"2022-03-11T06:32:42.208605Z","shell.execute_reply":"2022-03-11T06:33:18.535073Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Recurrent Autoencoders","metadata":{"id":"uDMfAWWTn2x7"}},{"cell_type":"markdown","source":"If you want to build an autoencoder for sequences, such as time series or text, then recurrent neural networks may be better suited than dense networks. Building a recurrent autoencoder is straightforward: **the encoder is typically a sequence-to-vector RNN** which compresses the input sequence down to a single vector. **The decoder is a vector-to-sequence RNN** that does the reverse:","metadata":{"id":"Q0Fa6noX5EGM"}},{"cell_type":"code","source":"recurrent_encoder = keras.models.Sequential([\n    keras.layers.LSTM(100, return_sequences=True, input_shape=[28, 28]),\n    keras.layers.LSTM(30)\n])\nrecurrent_decoder = keras.models.Sequential([\n    keras.layers.RepeatVector(28, input_shape=[30]),\n    keras.layers.LSTM(100, return_sequences=True),\n    keras.layers.TimeDistributed(keras.layers.Dense(28, activation=\"sigmoid\"))\n])\nrecurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])\nrecurrent_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(0.1),\n                     metrics=[rounded_accuracy])\n\nhistory = recurrent_ae.fit(X_train, X_train, epochs=5, validation_data=(X_valid, X_valid))","metadata":{"id":"g503xRD_n2x7","outputId":"31e6dfac-9337-4af7-fa56-dddf7f83bb10","execution":{"iopub.status.busy":"2022-03-11T06:33:18.537169Z","iopub.execute_input":"2022-03-11T06:33:18.541173Z","iopub.status.idle":"2022-03-11T06:34:34.276364Z","shell.execute_reply.started":"2022-03-11T06:33:18.541132Z","shell.execute_reply":"2022-03-11T06:34:34.275623Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"This recurrent autoencoder can process sequences of any length, with 28 dimensions per time step. Conveniently, **this means it can process Fashion MNIST images by treating each image as a sequence of rows: at each time step, the RNN will process a single row of 28 pixels.** Note that we use a `RepeatVector` layer as the first layer of the decoder, to ensure that its input\nvector gets fed to the decoder at each time step.","metadata":{"id":"fudYUaxT5gFy"}},{"cell_type":"code","source":"show_reconstructions(recurrent_ae)","metadata":{"id":"2XgTO34jn2x7","outputId":"3566f070-8961-4956-eeee-a40e81a119a0","execution":{"iopub.status.busy":"2022-03-11T06:34:34.277933Z","iopub.execute_input":"2022-03-11T06:34:34.278199Z","iopub.status.idle":"2022-03-11T06:34:35.583274Z","shell.execute_reply.started":"2022-03-11T06:34:34.278164Z","shell.execute_reply":"2022-03-11T06:34:35.582623Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"Z = recurrent_encoder.predict(X_valid)\nprint(Z.shape)\nN = Z.shape[0]\nZZ = np.reshape(Z, (N,-1))\nprint(ZZ.shape)\n\n\nplot_embeddings_tsne(X_valid, y_valid, ZZ)\nplt.tight_layout()\nplt.show()","metadata":{"id":"j7SIdCpd5qt6","outputId":"d263df69-430b-4b0b-f01e-4230c39099d3","execution":{"iopub.status.busy":"2022-03-11T06:34:35.586916Z","iopub.execute_input":"2022-03-11T06:34:35.589091Z","iopub.status.idle":"2022-03-11T06:35:09.367825Z","shell.execute_reply.started":"2022-03-11T06:34:35.589050Z","shell.execute_reply":"2022-03-11T06:35:09.367140Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Stacked denoising Autoencoder","metadata":{"id":"Ca8A5O-Pn2x7"}},{"cell_type":"markdown","source":"Another way to force the autoencoder to learn useful features is to add noise to its inputs, training it to recover the original, noise-free inputs. The noise can be pure Gaussian noise added to the inputs, or it can be randomly switched-off inputs, just like in dropout (Bernoulli)","metadata":{"id":"UC86f3R8n2x7"}},{"cell_type":"code","source":"# Add Gaussian noise\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ndenoising_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.GaussianNoise(0.2),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(30, activation=\"selu\")\n])\ndenoising_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\ndenoising_ae = keras.models.Sequential([denoising_encoder, denoising_decoder])\ndenoising_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n                     metrics=[rounded_accuracy])\nhistory = denoising_ae.fit(X_train, X_train, epochs=10,\n                           validation_data=(X_valid, X_valid))","metadata":{"id":"acJO4IeGn2x7","outputId":"48799637-ff55-45ae-8ae8-1202d86c234b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n\nnoise = keras.layers.GaussianNoise(0.2)\nshow_reconstructions(denoising_ae, noise(X_valid, training=True))\nplt.show()","metadata":{"id":"R7HmVlI4n2x7","outputId":"5f3f410d-78d6-4bfb-f8c4-0820f9679c74"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add Bernoulli dropout noise\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\ndropout_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(30, activation=\"selu\")\n])\ndropout_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[30]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\ndropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])\ndropout_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n                   metrics=[rounded_accuracy])\nhistory = dropout_ae.fit(X_train, X_train, epochs=10,\n                         validation_data=(X_valid, X_valid))","metadata":{"id":"Sn87go_pn2x8","outputId":"641d2464-dbca-4abe-cc3e-7e438199a74c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n\ndropout = keras.layers.Dropout(0.5)\nshow_reconstructions(dropout_ae, dropout(X_valid, training=True))\nsave_fig(\"dropout_denoising_plot\", tight_layout=False)","metadata":{"id":"eb7VLw7tn2x8","outputId":"8ec48f17-1bf8-4ee9-e184-b5df85331fa1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sparse Autoencoder","metadata":{"id":"5k5Xk6epn2x8"}},{"cell_type":"markdown","source":"Another kind of constraint that often leads to good feature extraction is sparsity: by adding an appropriate term to the cost function, the autoencoder is pushed to reduce the number of active neurons in the coding layer. For\nexample, it may be pushed to have on average only 5% significantly active neurons in the coding layer. This forces the autoencoder to represent each input as a combination of a small number of activations. As a result,\neach neuron in the coding layer typically ends up representing a useful feature.","metadata":{"id":"28nUFmRYn2x8"}},{"cell_type":"code","source":"# To visualize statistics of the hidden units adapted from https://github.com/probml/pml-book/tree/main/pml1/figure_notebooks\n\ndef plot_percent_hist(ax, data, bins):\n    counts, _ = np.histogram(data, bins=bins)\n    widths = bins[1:] - bins[:-1]\n    x = bins[:-1] + widths / 2\n    ax.bar(x, counts / len(data), width=widths*0.8)\n    ax.xaxis.set_ticks(bins)\n    ax.yaxis.set_major_formatter(mpl.ticker.FuncFormatter(\n        lambda y, position: \"{}%\".format(int(np.round(100 * y)))))\n    ax.grid(True)\n\ndef plot_activations_histogram2(encoder, height=1, n_bins=10, fname_base=\"\"):\n    X_valid_codings = encoder(X_valid).numpy()\n    activation_means = X_valid_codings.mean(axis=0)\n    mean = activation_means.mean()\n    bins = np.linspace(0, 1, n_bins + 1)\n\n    fig, ax1 = plt.subplots()\n    plot_percent_hist(ax1, X_valid_codings.ravel(), bins)\n    ax1.plot([mean, mean], [0, height], \"k--\", label=\"Overall Mean = {:.2f}\".format(mean))\n    ax1.legend(loc=\"upper center\", fontsize=14)\n    ax1.set_xlabel(\"Activation\")\n    ax1.set_ylabel(\"% Activations\")\n    ax1.axis([0, 1, 0, height])\n    fname_act = '{}-act.pdf'.format(fname_base)\n    #save_fig(fname_act)\n    plt.show()\n    \n    fig, ax2 = plt.subplots()\n    plot_percent_hist(ax2, activation_means, bins)\n    ax2.plot([mean, mean], [0, height], \"k--\", label=\"Overall Mean = {:.2f}\".format(mean))\n    ax2.set_xlabel(\"Neuron Mean Activation\")\n    ax2.set_ylabel(\"% Neurons\")\n    ax2.axis([0, 1, 0, height])\n    fname_act = '{}-neurons.pdf'.format(fname_base)\n    #save_fig(fname_act)\n    plt.show()\n\ndef plot_activations_heatmap(encoder, N=100):\n    X = encoder(X_valid).numpy()\n    plt.figure(figsize=(10,5))\n    plt.imshow(X[:N,:])\n    plt.colorbar()","metadata":{"id":"bTfRLuVh8tuk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple AE with sigmoid activations on the bottleneck\n    \ntf.random.set_seed(42)\nnp.random.seed(42)\n\nNhidden = 300 \nsimple_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(Nhidden, activation=\"sigmoid\"),\n])\nsimple_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[Nhidden]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nsimple_ae = keras.models.Sequential([simple_encoder, simple_decoder])\nsimple_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.),\n                  metrics=[rounded_accuracy])\nhistory = simple_ae.fit(X_train, X_train, epochs=10,\n                        validation_data=(X_valid, X_valid))","metadata":{"id":"mil4Ay0cn2x8","outputId":"4e6038c5-0474-40f4-b66d-be019ab1175a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_reconstructions(simple_ae)\nplot_activations_heatmap(simple_encoder)\nplot_activations_histogram2(simple_encoder, height=0.35, fname_base=\"ae-sparse-noreg\")\nplt.show()","metadata":{"id":"hD6tLqEkn2x8","outputId":"2bb45aeb-81f9-4564-fd09-1394647b8322"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three figures show neuron activity (in the bottleneck layer) for an autoencoder applied to Fashion MNIST. Heatmap of 300 neuron activations (columns) across 100 examples (rows). Histogram of activation levels derived from this heatmap. \nHistogram of the mean activation per neuron, averaged over all examples in the validation set. \n\nYou can see that values close to 0 or 1 are more frequent overall, which is consistent with the saturating nature of the sigmoid function. Also, you can see that most neurons have a mean activation close to 0.5. Both histograms tell us that each neuron tends to either fire close to 0 or 1, with about 50% probability each. However, some neurons fire almost all the time (right side of the histogram).\n\nA simple approach is to use the sigmoid activation function in the coding layer (to constrain the codings to values between 0 and 1), use a large coding layer (e.g., with 300 units), and add some ℓ regularization to the coding layer’s activations.","metadata":{"id":"gLY40YVG9YK8"}},{"cell_type":"code","source":"# Add L1 regularizer\n\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nsparse_l1_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(300, activation=\"sigmoid\"),\n    keras.layers.ActivityRegularization(l1=1e-3)  # Alternatively, you could add\n                                                  # activity_regularizer=keras.regularizers.l1(1e-3)\n                                                  # to the previous layer.\n])\nsparse_l1_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[300]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nsparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])\nsparse_l1_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n                     metrics=[rounded_accuracy])\nhistory = sparse_l1_ae.fit(X_train, X_train, epochs=10,\n                           validation_data=(X_valid, X_valid))","metadata":{"id":"iphzvC9on2x9","outputId":"505087c1-062f-4a76-9dfb-4072cbae5399"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `ActivityRegularization` layer just returns its inputs, but as a side effect it adds a training loss equal to the sum of absolute values of its inputs (this layer only has an effect during training). layer. This penalty will encourage the neural network to produce codings close to 0, but since it\nwill also be penalized if it does not reconstruct the inputs correctly, it will have to output at least a few nonzero values. Using the ℓ norm rather than the ℓ norm will push the neural network to preserve the most important codings while eliminating the ones that are not needed for the input image (rather than just reducing all codings).","metadata":{"id":"z2i9zEv3ADi9"}},{"cell_type":"code","source":"show_reconstructions(sparse_l1_ae)\nplot_activations_heatmap(sparse_l1_encoder)\nplot_activations_histogram2(sparse_l1_encoder, fname_base=\"ae-sparse-L1reg\")\n#save_fig(\"ae-sparse-L1reg-heatmap.pdf\")\nplt.show()","metadata":{"id":"Vc5-bUjvn2x9","outputId":"fb9547f6-737e-4a5b-9305-b949ecb74ef1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another approach, which often yields better results, is to measure the actual sparsity of the coding layer at each training iteration, and penalize the model when the measured sparsity differs from a target sparsity. We do so by\ncomputing the average activation of each neuron in the coding layer, over the whole training batch. Once we have the mean activation per neuron, we want to penalize the neurons that are too active, or not active enough, by adding a sparsity loss to the cost function. \n\nOnce we have computed the sparsity loss for each neuron in the coding layer, we sum up these losses and add the result to the cost function. In order to control the relative importance of the sparsity loss and the reconstruction\nloss, we can multiply the sparsity loss by a sparsity weight hyperparameter. If this weight is too high, the model will stick closely to the target sparsity, but it may not reconstruct the inputs properly, making the model useless. Conversely, if it is too low, the model will mostly ignore the sparsity objective and will not learn any interesting features.\n\nWe now have all we need to implement a sparse autoencoder based on the KL divergence. First, let’s create a custom regularizer to apply KL divergence regularization:","metadata":{"id":"ola1GkAsAhNm"}},{"cell_type":"code","source":"K = keras.backend\nkl_divergence = keras.losses.kullback_leibler_divergence\n\nclass KLDivergenceRegularizer(keras.regularizers.Regularizer):\n    def __init__(self, weight, target=0.1):\n        self.weight = weight\n        self.target = target\n    def __call__(self, inputs):\n        mean_activities = K.mean(inputs, axis=0)\n        return self.weight * (\n            kl_divergence(self.target, mean_activities) +\n            kl_divergence(1. - self.target, 1. - mean_activities))","metadata":{"id":"FIQCzi2mn2x-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n\nkld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)\nsparse_kl_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape=[28, 28]),\n    keras.layers.Dense(100, activation=\"selu\"),\n    keras.layers.Dense(300, activation=\"sigmoid\", activity_regularizer=kld_reg)\n])\nsparse_kl_decoder = keras.models.Sequential([\n    keras.layers.Dense(100, activation=\"selu\", input_shape=[300]),\n    keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n    keras.layers.Reshape([28, 28])\n])\nsparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])\nsparse_kl_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(learning_rate=1.0),\n              metrics=[rounded_accuracy])\nhistory = sparse_kl_ae.fit(X_train, X_train, epochs=10,\n                           validation_data=(X_valid, X_valid))","metadata":{"id":"YpzHOIz5n2x-","outputId":"ba44aeec-6e33-4b3f-c8b4-c0ccf86916b5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_reconstructions(sparse_kl_ae)\nplot_activations_heatmap(sparse_kl_encoder)\nplot_activations_histogram2(sparse_kl_encoder,  fname_base=\"ae-sparse-KLreg\")\n#save_fig(\"ae-sparse-KLreg-heatmap.pdf\")\nplt.show()","metadata":{"id":"2vhXVGZin2x-","outputId":"c04bbfd0-7694-4243-a5da-3b7d03a427d9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training this sparse autoencoder on Fashion MNIST, the activations of the neurons in the coding layer are mostly close to 0 (about 70% of all activations are lower than 0.1), and all neurons have a mean activation around 0.1 (about 90% of all neurons have a mean activation between 0.1 and 0.2)","metadata":{"id":"VZv_70qOBee5"}},{"cell_type":"markdown","source":"# Variational Autoencoder","metadata":{"id":"5M4zgZsin2x-"}},{"cell_type":"markdown","source":"We’re going to be implementing a VAE that can generate MNIST digits. It’s going to have three parts:\n\n* An encoder network that turns a real image into a mean and a variance in the\nlatent space\n* A sampling layer that takes such a mean and variance, and uses them to sample a random point from the latent space\n* A decoder network that turns points from the latent space back into images\n\nThe following listing shows the encoder network we’ll use, mapping images to the\nparameters of a probability distribution over the latent space. It’s a simple convnet that maps the input image x to two vectors, z_mean and z_log_var. One important detail is that we use strides for downsampling feature maps instead of max pooling. **Recall that, in general, strides are preferable to max pooling for any model that cares about information location**—that is to say, where stuff is in the image—and this one does, since it will have to produce an image encoding that can be used to reconstruct a valid image.","metadata":{"id":"m6zOZj_8FMVg"}},{"cell_type":"markdown","source":"### VAE encoder","metadata":{"id":"TuR_dbXIFrrw"}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\n# Dimensionality of the latent space: a 2D plane\nlatent_dim = 2 \n\nencoder_inputs = keras.Input(shape=(28, 28, 1))\nx = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\nx = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Flatten()(x)\nx = layers.Dense(16, activation=\"relu\")(x)\n# The input image ends up being encoded into these two parameters.\nz_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")","metadata":{"id":"N2pe_vmln2x_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder.summary()","metadata":{"id":"kcu6o7k9n2x_","outputId":"eb34dfbd-82cf-4747-ce4a-33ebf5867428"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Latent-space-sampling layer","metadata":{"id":"s2gxL44TG0Xj"}},{"cell_type":"markdown","source":"Next is the code for using `z_mean` and `z_log_var`, the parameters of the statistical distribution assumed to have produced input_img, to generate a latent space point z.","metadata":{"id":"ZVaLFsQRGHev"}},{"cell_type":"code","source":"class Sampler(layers.Layer):\n    def call(self, z_mean, z_log_var):\n        batch_size = tf.shape(z_mean)[0]\n        z_size = tf.shape(z_mean)[1]\n        # Draw a batch of random normal Apply the VAE vectors.\n        epsilon = tf.random.normal(shape=(batch_size, z_size))\n        # Apply the VAE sampling formula\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon","metadata":{"id":"z5jG_szUGDT3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### VAE decoder network, mapping latent space points to images","metadata":{"id":"wWleV6sZG2_g"}},{"cell_type":"code","source":"# Input where we’ll feed z\nlatent_inputs = keras.Input(shape=(latent_dim,))\n# Produce the same number of coefficients that we had at the level of the Flatten layer in the encoder.\nx = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\nx = layers.Reshape((7, 7, 64))(x)\nx = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\ndecoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n\ndecoder.summary()","metadata":{"id":"hDAVwRX7G6R4","outputId":"7cbfcf00-237d-40e6-e1ab-8d79026cecd9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let’s create the VAE model itself. Whenever you depart from classic supervised learning, it’s common to subclass the Model class and implement a custom `train_step()` to specify the new training logic","metadata":{"id":"1jW-lMzlG-k3"}},{"cell_type":"code","source":"class VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super().__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.sampler = Sampler()\n        # We use these metrics to keep track of the loss averages over each epoch.\n        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = keras.metrics.Mean(\n            name=\"reconstruction_loss\")\n        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n\n    # We list the metrics in the metrics property to enable the model to reset\n    # them after each epoch (or between multiple calls to fit()/evaluate()).\n    @property\n    def metrics(self):\n        return [self.total_loss_tracker,\n                self.reconstruction_loss_tracker,\n                self.kl_loss_tracker]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var = self.encoder(data)\n            z = self.sampler(z_mean, z_log_var)\n            reconstruction = decoder(z)\n            # We sum the reconstruction loss over the spatial\n            # dimensions (axes 1 and 2) and take its mean over the\n            # batch dimension.\n            reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                    keras.losses.binary_crossentropy(data, reconstruction),\n                    axis=(1, 2)\n                )\n            )\n            # Add the regularization term\n            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n        return {\n            \"total_loss\": self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }","metadata":{"id":"fRAwvHTQHCOn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we’re ready to instantiate and train the model on MNIST digits. Because the loss is taken care of in the custom layer, we don’t specify an external loss at compile time (`loss=None`), which in turn means we won’t pass target data during training","metadata":{"id":"Fed7JjAmJTRq"}},{"cell_type":"code","source":"(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n\n# We train on all MNIST digits, so we concatenate the training and test samples.\nmnist_digits = np.concatenate([x_train, x_test], axis=0)\nmnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n\nvae = VAE(encoder, decoder)\n\n# Note that we don’t pass a loss argument in compile(), since the loss\n# is already part of the train_step().\nvae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n\n# Note that we don’t pass targets in fit(), since train_step()\n# doesn’t expect any.\nvae.fit(mnist_digits, epochs=30, batch_size=128)","metadata":{"id":"wVafukYDHF5A","outputId":"a5bfbae5-adc5-4efd-bcf1-209de1ef28b0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the model is trained, we can use the decoder network to turn arbitrary latent space vectors into images.","metadata":{"id":"hrX4_otyJv2Q"}},{"cell_type":"code","source":"n = 30\ndigit_size = 28\nfigure = np.zeros((digit_size * n, digit_size * n))\n\ngrid_x = np.linspace(-1, 1, n)\ngrid_y = np.linspace(-1, 1, n)[::-1]\n\nfor i, yi in enumerate(grid_y):\n    for j, xi in enumerate(grid_x):\n        z_sample = np.array([[xi, yi]])\n        x_decoded = vae.decoder.predict(z_sample)\n        digit = x_decoded[0].reshape(digit_size, digit_size)\n        figure[\n            i * digit_size : (i + 1) * digit_size,\n            j * digit_size : (j + 1) * digit_size,\n        ] = digit\n\nplt.figure(figsize=(15, 15))\nstart_range = digit_size // 2\nend_range = n * digit_size + start_range\npixel_range = np.arange(start_range, end_range, digit_size)\nsample_range_x = np.round(grid_x, 1)\nsample_range_y = np.round(grid_y, 1)\nplt.xticks(pixel_range, sample_range_x)\nplt.yticks(pixel_range, sample_range_y)\nplt.xlabel(\"z[0]\")\nplt.ylabel(\"z[1]\")\nplt.axis(\"off\")\nplt.imshow(figure, cmap=\"Greys_r\")","metadata":{"id":"ylhQD0QgHH6n","outputId":"3b070091-e56a-4c62-d060-ebf39b2e2480"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The grid of sampled digits shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a\npath through latent space. Specific directions in this space have a meaning: for example, there are directions for “five-ness,” “one-ness,” and so on.","metadata":{"id":"EyyeLDC-JxYo"}},{"cell_type":"markdown","source":"# Generative Adversarial Networks","metadata":{"id":"ZEqMKTeDn2yA"}},{"cell_type":"code","source":"(x_train, _), (x_test, _) = keras.datasets.fashion_mnist.load_data()\nx_train = np.concatenate([x_train, x_test], axis=0)\nx_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\nx_train_dcgan = x_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale\n#batch_size = 32\n#dataset = tf.data.Dataset.from_tensor_slices(x_train_dcgan).shuffle(1000)\n#dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)","metadata":{"id":"WCDRCDdpn2yA","execution":{"iopub.status.busy":"2022-03-11T07:51:11.187770Z","iopub.execute_input":"2022-03-11T07:51:11.188028Z","iopub.status.idle":"2022-03-11T07:51:11.762441Z","shell.execute_reply.started":"2022-03-11T07:51:11.187999Z","shell.execute_reply":"2022-03-11T07:51:11.761637Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfor x in dataset:\n    plt.axis(\"off\")\n    plt.imshow((x.numpy() * 255).astype(\"int32\")[0], cmap=\"binary\")\n    break","metadata":{"id":"KLtBoUMnb8yf","outputId":"b9ed4913-da2d-4f82-fa22-fcd9e42c19e3","execution":{"iopub.status.busy":"2022-03-11T07:47:15.537505Z","iopub.execute_input":"2022-03-11T07:47:15.538322Z","iopub.status.idle":"2022-03-11T07:47:15.869790Z","shell.execute_reply.started":"2022-03-11T07:47:15.538250Z","shell.execute_reply":"2022-03-11T07:47:15.869036Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"### The discriminator","metadata":{"id":"97G10rEALxlS"}},{"cell_type":"markdown","source":"First, we’ll develop a discriminator model that takes as input a candidate image\n(real or synthetic) and classifies it into one of two classes: “generated image” or “real image that comes from the training set.” One of the many issues that commonly **arise with GANs is that the generator gets stuck with generated images that look like noise. A possible solution is to use dropout in the discriminator,** so that’s what we will do here.","metadata":{"id":"M1BN5Fp-Mj0K"}},{"cell_type":"code","source":"from tensorflow.keras import layers\n\ndiscriminator = keras.Sequential(\n    [\n        keras.Input(shape=(28, 28, 1)),\n        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.4),\n        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Dropout(0.4),\n        layers.Flatten(),\n        layers.Dense(1, activation=\"sigmoid\"),\n    ],\n    name=\"discriminator\",\n)","metadata":{"id":"WnULOjfFn2yA","execution":{"iopub.status.busy":"2022-03-11T07:51:14.983275Z","iopub.execute_input":"2022-03-11T07:51:14.983869Z","iopub.status.idle":"2022-03-11T07:51:15.027770Z","shell.execute_reply.started":"2022-03-11T07:51:14.983823Z","shell.execute_reply":"2022-03-11T07:51:15.027081Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"discriminator.summary()","metadata":{"id":"wOczDglbn2yA","outputId":"14219144-4e16-4641-e9b0-9743ef7dca97","execution":{"iopub.status.busy":"2022-03-11T07:51:15.277651Z","iopub.execute_input":"2022-03-11T07:51:15.277851Z","iopub.status.idle":"2022-03-11T07:51:15.287739Z","shell.execute_reply.started":"2022-03-11T07:51:15.277827Z","shell.execute_reply":"2022-03-11T07:51:15.286859Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"markdown","source":"### The generator","metadata":{"id":"lMnToYwmL3Ha"}},{"cell_type":"markdown","source":"Next, let’s develop a generator model that turns a vector (from the latent space—during training it will be sampled at random) into a candidate image.","metadata":{"id":"ul_16tnvM-CU"}},{"cell_type":"code","source":"latent_dim = 100\n\n# We use LeakyReLU as our activation\ngenerator = keras.Sequential(\n    [\n        keras.Input(shape=(latent_dim,)),\n        layers.Dense(7 * 7 * 128),\n        layers.Reshape((7, 7, 128)),\n        keras.layers.BatchNormalization(),\n        layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", activation=\"selu\"),\n        keras.layers.BatchNormalization(),\n        layers.Conv2DTranspose(1, kernel_size=4, strides=2, padding=\"same\", activation=\"tanh\"),\n    ],\n    name=\"generator\",\n)","metadata":{"id":"legTdcX6n2yB","execution":{"iopub.status.busy":"2022-03-11T07:51:19.429917Z","iopub.execute_input":"2022-03-11T07:51:19.430178Z","iopub.status.idle":"2022-03-11T07:51:19.493063Z","shell.execute_reply.started":"2022-03-11T07:51:19.430151Z","shell.execute_reply":"2022-03-11T07:51:19.492288Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"generator.summary()","metadata":{"id":"D9F6_5FMn2yB","outputId":"deee1b73-e687-471f-9328-c0e5d1a55672","execution":{"iopub.status.busy":"2022-03-11T07:51:20.056516Z","iopub.execute_input":"2022-03-11T07:51:20.056973Z","iopub.status.idle":"2022-03-11T07:51:20.068500Z","shell.execute_reply.started":"2022-03-11T07:51:20.056939Z","shell.execute_reply":"2022-03-11T07:51:20.067681Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"markdown","source":"### The adversarial network","metadata":{"id":"qDXGYWoLMATy"}},{"cell_type":"markdown","source":"Finally, we’ll set up the GAN, which chains the generator and the discriminator.When trained, this model will move the generator in a direction that improves its ability to fool the discriminator. This model turns latent-space points into a classification decision—“fake” or “real”—and it’s meant to be trained with labels that are always “these are real images.” So training gan will update the weights of generator in a way that makes discriminator more likely to predict “real” when looking at fake images.\n\nTo recapitulate, this is what the training loop looks like schematically. For each epoch, you do the following:\n\n1. Draw random points in the latent space (random noise).\n2. Generate images with generator using this random noise.\n3. Mix the generated images with real ones.\n4. Train discriminator using these mixed images, with corresponding targets: either “real” (for the real images) or “fake” (for the generated images).\n5. Draw new random points in the latent space.\n6. Train generator using these random vectors, with targets that all say “these are real images.” This updates the weights of the generator to move them toward\ngetting the discriminator to predict “these are real images” for generated\nimages: this trains the generator to fool the discriminator.\n\nLet’s implement it. Like in our VAE example, we’ll use a Model subclass with a custom `train_step()`. Note that we’ll use two optimizers (one for the generator and one for the discriminator), so we will also override `compile()` to allow for passing two optimizers.","metadata":{"id":"HCbTzO-Kox0z"}},{"cell_type":"code","source":"class GAN(keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super().__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super(GAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, real_images):\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim))\n        generated_images = self.generator(random_latent_vectors)\n        combined_images = tf.concat([generated_images, real_images], axis=0)\n        labels = tf.concat(\n            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))],\n            axis=0\n        )\n        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        random_latent_vectors = tf.random.normal(\n            shape=(batch_size, self.latent_dim))\n\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(\n                self.generator(random_latent_vectors))\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(\n            zip(grads, self.generator.trainable_weights))\n\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\"d_loss\": self.d_loss_metric.result(),\n                \"g_loss\": self.g_loss_metric.result()}","metadata":{"id":"gTz-spN0n2yB","execution":{"iopub.status.busy":"2022-03-11T07:51:24.030358Z","iopub.execute_input":"2022-03-11T07:51:24.030690Z","iopub.status.idle":"2022-03-11T07:51:24.049601Z","shell.execute_reply.started":"2022-03-11T07:51:24.030649Z","shell.execute_reply":"2022-03-11T07:51:24.048437Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"class GANMonitor(keras.callbacks.Callback):\n    def __init__(self, num_img=3, latent_dim=128):\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n\n    def on_epoch_end(self, epoch, logs=None):\n        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n        generated_images = self.model.generator(random_latent_vectors)\n        generated_images *= 255\n        plot_multiple_images(generated_images.numpy(), 8) \n        plt.show()\n        #for i in range(self.num_img):\n        #    img = keras.utils.array_to_img(generated_images[i])\n        #    img.save(f\"generated_img_{epoch:03d}_{i}.png\")","metadata":{"id":"Lyc_Ah69MDJ9","execution":{"iopub.status.busy":"2022-03-11T07:51:25.484851Z","iopub.execute_input":"2022-03-11T07:51:25.485436Z","iopub.status.idle":"2022-03-11T07:51:25.493416Z","shell.execute_reply.started":"2022-03-11T07:51:25.485398Z","shell.execute_reply":"2022-03-11T07:51:25.492382Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"epochs = 30\n\ngan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\ngan.compile(\n    d_optimizer=keras.optimizers.RMSprop(),\n    g_optimizer=keras.optimizers.RMSprop(),\n    loss_fn=keras.losses.BinaryCrossentropy(),\n)\n\ngan.fit(\n    x_train_dcgan, epochs=epochs, callbacks=[GANMonitor(num_img=32, latent_dim=latent_dim)], batch_size=32\n)","metadata":{"id":"O6csRGMQMEoM","outputId":"310c1148-6559-4098-f529-e02d5ee57035","execution":{"iopub.status.busy":"2022-03-11T07:51:53.488077Z","iopub.execute_input":"2022-03-11T07:51:53.488737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}